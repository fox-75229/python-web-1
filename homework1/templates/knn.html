{% extends "base.html" %}

{% block title %}k-近鄰分類：聽聽鄰居怎麼說{% endblock %}

{% block content %}
<div class="content-section">
    <h1>k-近鄰演算法：最民主的分類法——少數服從多數！</h1>
    <hr>
    <p>
        k-近鄰 (k-NN) 演算法是個非常符合直覺的模型，它的核心哲學是：「想知道你是誰？那就看看你身邊最親近的朋友們是誰。」當一個身份不明的新數據點出現時，k-NN
        會做兩件事：首先，它會測量這個新點與數據庫中所有舊點的距離；接著，它會找出離它最近的「k」個鄰居。最後，它會發起一場民主投票：這 k 個鄰居中，哪個類別的票數最多，新數據點就屬於哪個類別！
    </p>

    <h3>關鍵的神秘數字 "k"</h3>
    <p>
        "k" 的選擇至關重要！如果 k 太小（例如 k=1），模型就會很容易被一兩個奇怪的鄰居（噪聲）誤導。但如果 k 太大，例如問了全村的人，那又可能會忽略掉小群體的獨特性，讓預測變得過於籠統。選擇一個恰當的 k
        值，就像在交友中找到核心朋友圈一樣，是 k-NN 成功的關鍵。
    </p>

    <h3>優點與挑戰</h3>
    <ul>
        <li><strong>優點：</strong> 原理超級簡單，完全不需要「訓練」過程，拿到新數據就能直接上工。</li>
        <li><strong>挑戰：</strong> 因為每次預測都要跟所有人比一次距離，當數據量達到百萬、千萬級別時，它會變得非常非常慢，就像一個超級謹慎的社交達人，認識新朋友前都要把所有人都了解一遍。</li>
    </ul>
</div>
{% endblock %}